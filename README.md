# ğŸ›ï¸ E-commerce Transactions Data Pipeline using AWS Glue, Lambda & Athena

---

## ğŸ“Œ Project Overview

This project demonstrates the creation of an **event-driven ETL pipeline** for processing **e-commerce transaction data** using **AWS Lambda, Glue, Athena, and S3**.

The pipeline automatically triggers data transformation when a new file is uploaded to **Amazon S3**, processes the data using **Glue ETL**, updates schema using **Glue Crawlers**, and makes the transformed data queryable using **Amazon Athena**.

---

## ğŸ—‚ï¸ Table of Contents

- [Project Architecture](#-project-architecture)
- [Dataset Description](#-dataset-description)
- [AWS Services Used](#-aws-services-used)
- [Project Workflow](#-project-workflow)
  - [1. Raw Data Ingestion](#1-raw-data-ingestion)
  - [2. Lambda Trigger](#2-lambda-trigger)
  - [3. Glue Crawlers](#3-glue-crawlers)
  - [4. Glue ETL Job](#4-glue-etl-job)
  - [5. Athena Querying](#5-athena-querying)
- [Transformed Dataset Schema](#-transformed-dataset-schema)
- [Lambda Function Details](#-lambda-function-details)
- [SQL Queries](#-sql-queries)
- [Folder Structure](#-folder-structure)
- [How to Run](#-how-to-run)
- [Future Improvements](#-future-improvements)
- [Author](#-author)

---

## ğŸ—ï¸ Project Architecture

```plaintext
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Raw Dataset      â”‚
          â”‚  (S3 Bucket)       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  (PUT/POST Event)
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Lambda Function   â”‚
          â”‚ trigger-glue-etl   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         Starts Glue ETL Job
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Glue ETL Job       â”‚
        â”‚  (pandas_etl_job)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        Writes Transformed Data
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Transformed Dataset  â”‚
        â”‚     (S3 Bucket)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    Starts Crawler for Schema Update
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Glue Crawler #2      â”‚
        â”‚ curated_to_analysis  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Amazon Athena        â”‚
        â”‚ Query Transformed    â”‚
        â”‚ Data for Analysis    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Dataset Description

The dataset contains e-commerce transaction records with the following key attributes:

- **Transaction ID**: Unique identifier for each transaction
- **Customer Information**: Customer ID, demographics
- **Product Details**: Product ID, category, price
- **Transaction Details**: Timestamp, quantity, total amount
- **Location Data**: Geographic information for analysis

---

## â˜ï¸ AWS Services Used

| Service | Purpose |
|---------|----------|
| **Amazon S3** | Data storage for raw and transformed datasets |
| **AWS Lambda** | Event-driven trigger for ETL pipeline |
| **AWS Glue** | Data cataloging and ETL transformations |
| **Amazon Athena** | Serverless querying of transformed data |
| **AWS IAM** | Identity and access management |

---

## ğŸ”„ Project Workflow

### 1. Raw Data Ingestion
- Raw e-commerce transaction data is uploaded to the **source S3 bucket**
- Data format: CSV files with transaction records
- S3 bucket configured with event notifications

### 2. Lambda Trigger
- **Lambda function** (`trigger-glue-etl`) is automatically invoked on S3 PUT/POST events
- Function validates the uploaded file and initiates the ETL process
- Error handling and logging implemented for monitoring

### 3. Glue Crawlers
- **Crawler #1**: Scans raw data bucket to update schema in Glue Data Catalog
- **Crawler #2**: Scans transformed data bucket after ETL completion
- Automatic schema detection and table creation

### 4. Glue ETL Job
- **ETL Job** (`pandas_etl_job`) processes the raw data
- Data transformations include:
  - Data cleaning and validation
  - Type conversions and formatting
  - Aggregations and calculations
  - Partitioning for optimal querying

### 5. Athena Querying
- Transformed data becomes queryable through **Amazon Athena**
- SQL-based analytics and reporting
- Integration with visualization tools

---

## ğŸ—ƒï¸ Transformed Dataset Schema

| Column | Type | Description |
|--------|------|-------------|
| `transaction_id` | string | Unique transaction identifier |
| `customer_id` | string | Customer unique identifier |
| `product_id` | string | Product unique identifier |
| `product_category` | string | Product category classification |
| `transaction_date` | date | Date of transaction |
| `transaction_amount` | double | Total transaction amount |
| `quantity` | int | Number of items purchased |
| `customer_location` | string | Customer geographic location |
| `payment_method` | string | Payment method used |

---

## âš¡ Lambda Function Details

### Function Configuration
- **Runtime**: Python 3.9
- **Memory**: 128 MB
- **Timeout**: 5 minutes
- **Trigger**: S3 Event Notification

### Key Features
- Automatic Glue job triggering
- Error handling and retry logic
- CloudWatch logging integration
- Environment variable configuration

```python
def lambda_handler(event, context):
    # Extract S3 event details
    # Validate file format
    # Trigger Glue ETL job
    # Handle errors and logging
    pass
```

---

## ğŸ” SQL Queries

### Sample Analytical Queries

#### 1. Top 5 Best Selling Products
```sql
SELECT product_name,
       SUM(quantity) AS total_quantity_sold
FROM "AwsDataCatalog"."ecommerce_db"."curated_for_analysiscurated"
GROUP BY product_name
ORDER BY total_quantity_sold DESC
LIMIT 5;
```

#### 2. Monthly Sales Trends
```sql
SELECT year,
       month,
       ROUND(SUM(total_price),2) AS monthly_revenue
FROM "AwsDataCatalog"."ecommerce_db"."curated_for_analysiscurated"
GROUP BY year, month
ORDER BY year, month;
```

#### 3. Highest Revenue Generating Categories
```sql
SELECT category,
       ROUND(SUM(total_price),2) AS total_category_revenue
FROM "AwsDataCatalog"."ecommerce_db"."curated_for_analysiscurated"
GROUP BY category
ORDER BY total_category_revenue DESC;
```

---

## ğŸ“ Folder Structure

```
Ecommerce_Transactions_Data_Pipeline/
â”œâ”€â”€ lambda_functions/
â”‚   â”œâ”€â”€ trigger-glue-etl.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ pandas_etl_job.py
â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ all_athena_queries/
â”‚   â”‚   â””â”€â”€ athena_queries.sql
â”‚   â””â”€â”€ Individual_queries/
â”‚       â””â”€â”€ avg_transactions......etc
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ecommerce_transactions_test.csv
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

### Prerequisites
- AWS Account with appropriate permissions
- AWS CLI configured
- Python 3.9+ installed

### Deployment Steps

1. **Set up S3 Buckets**
   ```bash
   aws s3 mb s3://your-raw-data-bucket
   aws s3 mb s3://your-transformed-data-bucket
   ```

2. **Deploy Lambda Function**
   - Create Lambda function with provided code
   - Configure S3 event trigger
   - Set up IAM roles and permissions

3. **Configure Glue Components**
   - Create Glue database
   - Set up crawlers for both buckets
   - Deploy ETL job script

4. **Test the Pipeline**
   - Upload sample CSV file to raw data bucket
   - Monitor Lambda and Glue job execution
   - Query results in Athena

### Monitoring and Troubleshooting
- Check CloudWatch logs for Lambda function
- Monitor Glue job execution status
- Verify data quality in transformed bucket

---

## ğŸ”® Future Improvements

- [ ] **Real-time Processing**: Implement Kinesis for streaming data
- [ ] **Data Quality Checks**: Add automated data validation rules
- [ ] **Cost Optimization**: Implement S3 lifecycle policies
- [ ] **Monitoring Dashboard**: Create CloudWatch dashboard
- [ ] **Error Notifications**: Set up SNS alerts for failures
- [ ] **Data Lineage**: Implement data lineage tracking
- [ ] **Security Enhancements**: Add data encryption at rest
- [ ] **Performance Optimization**: Implement data partitioning strategies

---

## ğŸ‘¨â€ğŸ’» Author

**Nivin24**
- GitHub: [@Nivin24](https://github.com/Nivin24)
- Project: E-commerce Transactions Data Pipeline

---

*This project demonstrates modern data engineering practices using AWS cloud services for scalable, event-driven data processing.*
